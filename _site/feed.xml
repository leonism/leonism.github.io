<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-07-21T08:25:25+00:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">DgPond.Com</title><subtitle>Data Driven Marketing, Machine Learning, Growth Hacking and Everything Sit in Between!</subtitle><author><name>Gerry Leo Nugroho</name></author><entry><title type="html">FIFAâ€™s Data Mining &amp;amp; Unsupervised Clustering Techniques with Dataiku</title><link href="http://localhost:4000/FIFA-data-mining-unsupervised-clustering-dataiku/" rel="alternate" type="text/html" title="FIFAâ€™s Data Mining &amp;amp; Unsupervised Clustering Techniques with Dataiku" /><published>2020-06-29T01:25:23+00:00</published><updated>2020-06-29T01:25:23+00:00</updated><id>http://localhost:4000/FIFA-data-mining-unsupervised-clustering-dataiku</id><content type="html" xml:base="http://localhost:4000/FIFA-data-mining-unsupervised-clustering-dataiku/">&lt;style&gt;.aligncenter {text-align: center;}&lt;/style&gt;

&lt;h3 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h3&gt;
&lt;p&gt;If there&amp;#39;s one thing similar about an interesting dataset and a good rerun football&amp;#39;s match on TV, is that they&amp;#39;re both doing pretty excellent job at keeping everyone&amp;#39;s safe at home during this time of the pandemic. And in all honesty, I&amp;#39;m not a data-scientist, nor a dev guru. I just recently got myself exposed to  &lt;strong&gt;Machine Learning, Data Mining&lt;/strong&gt;  and  &lt;strong&gt;Artificial Intelligent&lt;/strong&gt;  in general, while doing them in both  &lt;strong&gt;Dataiku&lt;/strong&gt; and  &lt;strong&gt;Python (Pandas, NumPy and SciKit&lt;/strong&gt; libraries&lt;strong&gt;)&lt;/strong&gt;, somewhere a little over then 3 months period of time. And without any further ado, here&amp;#39;s my take to the FIFA conundrum&amp;#39;s challenge.&lt;/p&gt;

&lt;h3 id=&quot;2-dataflow&quot;&gt;2. Dataflow&lt;/h3&gt;
&lt;p&gt;And since the challenge is not to  &lt;strong&gt;&amp;#39;predict&amp;#39;&lt;/strong&gt;  any variables, rather to &amp;#39;&lt;strong&gt;group&amp;#39;&lt;/strong&gt; or &amp;#39;&lt;strong&gt;cluster&amp;#39;&lt;/strong&gt;  the existing dataset from the player&amp;#39;s skillsets, in reflect to their  &lt;strong&gt;wages rate&lt;/strong&gt;. Here&amp;#39;s what my current flow would look like, and don&amp;#39;t bother much on the 2 additional datasets, as they&amp;#39;re merely exported from the existing model, so that I may explore them further later on. And to follow along, here&amp;#39;s the link to download the dataset in my 
&lt;a href=&quot;https://github.com/leonism/dataiku-FIFA/blob/master/uploads/Conundrum_13_Data/Conundrum_13_Data.csv&quot;&gt;Github repository&lt;/a&gt;.&lt;/p&gt;

&lt;p class=&quot;aligncenter&quot;&gt;&lt;img data-src=&quot;/img/posts/fifa-dataiku/workflow-diagram.png&quot; src=&quot;/img/posts/fifa-dataiku/workflow-diagram.png&quot; class=&quot;img-fluid&quot; alt=&quot;dataiku-fifa-analysis&quot; style=&quot;cursor: pointer;&quot;&gt;&lt;/p&gt;

&lt;h3 id=&quot;3-prepare-recipes&quot;&gt;3. Prepare Recipes&lt;/h3&gt;
&lt;p&gt;And here&amp;#39;s how I go about on the prepare recipes, nothing out of the ordinary. Just converting &lt;strong&gt;&lt;em&gt;categorical&lt;/em&gt;&lt;/strong&gt; to &lt;strong&gt;&lt;em&gt;numerical&lt;/em&gt;&lt;/strong&gt; values, through the &lt;strong&gt;&lt;em&gt;one-hot encoding&lt;/em&gt;&lt;/strong&gt; and filling up the &amp;#39;&lt;strong&gt;&lt;em&gt;NaN&lt;/em&gt;&lt;/strong&gt;&amp;#39; with median values, while grouping them to have better clarity, if ever the need occur for me to go back and revise anything again for future reference.&lt;/p&gt;

&lt;p class=&quot;aligncenter&quot;&gt;&lt;img data-src=&quot;/img/posts/fifa-dataiku/prepare-recipe.jpg&quot; src=&quot;/img/posts/fifa-dataiku/prepare-recipe.jpg&quot; class=&quot;img-fluid&quot; alt=&quot;prepare recipe&quot; style=&quot;cursor: pointer;&quot;&gt;&lt;/p&gt;


&lt;h3 id=&quot;4-data-modeling-training&quot;&gt;4. Data Modeling &amp;amp; Training&lt;/h3&gt;
&lt;p&gt;While on the  &lt;strong&gt;modeling&lt;/strong&gt;  and  &lt;strong&gt;training&lt;/strong&gt;  steps, I choose the &amp;#39;&lt;strong&gt;&lt;em&gt;Interactive Clustering&lt;/em&gt;&lt;/strong&gt;&amp;#39; machine-learning algorithm, which in returned, delivered me a sufficient scoring value. As a side note, Dataiku provide you with various Machine Learning Algorithm according to your prediction methods requirement. Be it for  &lt;strong&gt;Supervised Learning&lt;/strong&gt;  (develop predictive model based on both input and output data) and &lt;strong&gt;Unsupervised Learning (&lt;/strong&gt;group and interpret data based only on input data). And for the article alone, will be utilizing the later version of Unsupervised Learning for data mining/clustering.&lt;/p&gt;

&lt;p class=&quot;aligncenter&quot;&gt;&lt;img data-src=&quot;/img/posts/fifa-dataiku/model-results.png&quot; src=&quot;/img/posts/fifa-dataiku/model-results.png&quot; class=&quot;img-fluid&quot; alt=&quot;dataiku-data-modeling&quot; style=&quot;cursor: pointer;&quot;&gt;&lt;/p&gt;

&lt;h3 id=&quot;5-data-clustering&quot;&gt;5. Data Clustering&lt;/h3&gt;
&lt;p&gt;On to the clustering variables name, I simply identified them in the grading manner, starting from &lt;strong&gt;&amp;#39;&lt;em&gt;Grading A&lt;/em&gt;&amp;#39;&lt;/strong&gt;, as the most top-knot performer, all the way down to the least performing one marked with &lt;strong&gt;&amp;#39;&lt;em&gt;Grading E&lt;/em&gt;&amp;#39;&lt;/strong&gt;.&lt;/p&gt;

&lt;p class=&quot;aligncenter&quot;&gt;&lt;img data-src=&quot;/img/posts/fifa-dataiku/grading-classification3.jpg&quot; src=&quot;/img/posts/fifa-dataiku/grading-classification3.jpg&quot; class=&quot;img-fluid&quot; alt=&quot;dataiku-grading-classification&quot; style=&quot;cursor: pointer;&quot;&gt;&lt;/p&gt;

&lt;h3 id=&quot;6-cluster-plot&quot;&gt;6. Cluster Plot&lt;/h3&gt;
&lt;p&gt;And here&amp;#39;s how my cluster plot would look like, obviously the better the grade, the least volume of players getting included in them.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Acceleration x Wage&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;aligncenter&quot;&gt;&lt;img data-src=&quot;/img/posts/fifa-dataiku/acc-wages.png&quot; src=&quot;/img/posts/fifa-dataiku/acc-wages.png&quot; alt=&quot;dataiku-Acceleration-Wage&quot; class=&quot;img-fluid&quot; style=&quot;cursor: pointer;&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sliding Tackle x Wage&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;aligncenter&quot;&gt;&lt;img data-src=&quot;/img/posts/fifa-dataiku/sliding-tackle-wages.png&quot; src=&quot;/img/posts/fifa-dataiku/sliding-tackle-wages.png&quot; class=&quot;img-fluid&quot; alt=&quot;dataiku-Tackle-Wage&quot; style=&quot;cursor: pointer;&quot;&gt;&lt;/p&gt;

&lt;h3 id=&quot;7-grading-variables&quot;&gt;7. Grading Variables&lt;/h3&gt;
&lt;p&gt;And for sure, those who sit at the &lt;strong&gt;Grading A&lt;/strong&gt; level would stand above the average threshold measurements (though, that&amp;#39;s not always the case with other included variables, which I&amp;#39;m about to show down below).&lt;/p&gt;
&lt;p class=&quot;aligncenter&quot;&gt;&lt;img data-src=&quot;/img/posts/fifa-dataiku/variable-significant.jpg&quot; src=&quot;/img/posts/fifa-dataiku/variable-significant.jpg&quot; alt=&quot;variable-significant&quot; class=&quot;img-fluid&quot; style=&quot;cursor: pointer;&quot;&gt;&lt;/p&gt;

&lt;h3 id=&quot;8-values-proposition&quot;&gt;8. Values Proposition&lt;/h3&gt;
&lt;p&gt;And coming back again to the initial question, &lt;code&gt;&amp;quot;Creating a flow that outputs a value proposition in term of the wages&amp;quot;&lt;/code&gt;. I think I didn&amp;#39;t include the players name and their nationalities in my modeling for a couple of reasons. In my opinions, those two variables are just way too subjective to get included. In a sense, you could be a top-knot player, regardless of what your &amp;#39;Names&amp;#39; would sound like, and of course your &amp;#39;Nationalities&amp;#39;.&lt;/p&gt;

&lt;p&gt;So I&amp;#39;ve done the DSS flow diagram, while the followings are my list of &amp;#39;value proposition&amp;#39; that contributed of being one &amp;#39;Grading-A&amp;#39; player in the field.&lt;/p&gt;
&lt;p class=&quot;aligncenter&quot;&gt;&lt;img data-src=&quot;/img/posts/fifa-dataiku/top-5-values-proposition.png&quot; src=&quot;/img/posts/fifa-dataiku/top-5-values-proposition.png&quot; alt=&quot;top-5-values-proposition&quot; class=&quot;img-fluid&quot; style=&quot;cursor: pointer;&quot;&gt;&lt;/p&gt;

&lt;h3 id=&quot;9-top-5-values-proposition&quot;&gt;9. Top 5 Values Proposition&lt;/h3&gt;
&lt;p class=&quot;aligncenter&quot;&gt;&lt;img data-src=&quot;/img/posts/fifa-dataiku/values-by-distribution.png&quot; src=&quot;/img/posts/fifa-dataiku/values-by-distribution.png&quot; alt=&quot;values-by-distribution&quot; class=&quot;img-fluid&quot; style=&quot;cursor: pointer;&quot;&gt;&lt;/p&gt;

&lt;h3 id=&quot;10-top-5-values-proposition-by-distribution&quot;&gt;10. Top 5 Values Proposition By Distribution&lt;/h3&gt;
&lt;p class=&quot;aligncenter&quot;&gt;&lt;img data-src=&quot;/img/posts/fifa-dataiku/values-by-distribution-grading.png&quot; src=&quot;/img/posts/fifa-dataiku/values-by-distribution-grading.png&quot; alt=&quot;values-by-distribution-grading&quot; class=&quot;img-fluid&quot; style=&quot;cursor: pointer;&quot;&gt;&lt;/p&gt;

&lt;h3 id=&quot;11-correlation-matrix&quot;&gt;11. Correlation Matrix&lt;/h3&gt;
&lt;p&gt;The very first correlation analysis consists of plotting the &lt;em&gt;Correlation matrix&lt;/em&gt; for numerical variables. For each couple of numerical variables, this computes the &amp;quot;strength&amp;quot; of the correlation (called the Pearson coefficient):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1.0 means a perfect correlation&lt;/li&gt;
&lt;li&gt;0.0 means no correlation&lt;/li&gt;
&lt;li&gt;-1.0 means a perfect &amp;quot;inverse&amp;quot; correlation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since it does not really make sense to print this correlation plot for hundred of variables, we are restricting it to the first 50 numerical variables of the dataset.&lt;/p&gt;
&lt;p class=&quot;aligncenter&quot;&gt;
&lt;img src=&quot;/img/posts/fifa-dataiku/correlation-matrix.png&quot; alt=&quot;correlation-matrix&quot; class=&quot;img-fluid&quot; style=&quot;cursor: pointer;&quot;&gt;
&lt;img src=&quot;/img/posts/fifa-dataiku/correlation-matrix-2.png&quot; alt=&quot;correlation-matrix-2&quot; class=&quot;img-fluid&quot; style=&quot;cursor: pointer;&quot;&gt;
&lt;/p&gt;

&lt;h3 id=&quot;11-jupyter-notebook&quot;&gt;11. Jupyter Notebook&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Notes:&lt;/em&gt; Here are the links to the Python - Jupyter Notebook edition for the analysis process coming from Dataiku.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&quot;https://github.com/leonism/dataiku-FIFA/blob/master/ipython_notebooks/Correlations%20analysis%20on%20Conundrum_13_Data_prepared%20(admin).ipynb&quot;&gt;Correlation's analysis.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;https://github.com/leonism/dataiku-FIFA/blob/master/ipython_notebooks/Correlations%20analysis%20on%20Conundrum_13_Data_prepared_scored%20(admin).ipynb&quot;&gt;Correlations analysis scored.ipynb)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/leonism/dataiku-FIFA/blob/master/ipython_notebooks/High%20dimensional%20data%20visualization%20(t-SNE)%20on%20Conundrum_13_Data_prepared_scored%20(admin).ipynb&quot;&gt;High dimensional (t-SNE)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/leonism/dataiku-FIFA/blob/master/ipython_notebooks/PCA%20on%20Conundrum_13_Data_prepared_scored%20(admin).ipynb&quot;&gt;PCA&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/leonism/dataiku-FIFA/blob/master/ipython_notebooks/Statistics%20and%20tests%20on%20multiple%20populations%20on%20Conundrum_13_Data_prepared_scored%20(admin).ipynb&quot;&gt;Statistics and tests on a single population&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/leonism/dataiku-FIFA/blob/master/ipython_notebooks/Topic%20modeling%20on%20Conundrum_13_Data_prepared_scored%20(admin).ipynb&quot;&gt; Statistics and tests on multiple populations&quot; &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you wish to load the whole Project files into your working Dataiku&amp;#39;s project directory, download the whole files required at my GitHub repo as a single Zip file, and load them through the main interface.&lt;/p&gt;
&lt;p&gt;Been enjoying exploring this dataset for sure, and certainly it was fun doing it, stays safe everyone and leave a thumbs-up if you like the article and found this useful. ðŸ˜Š&lt;/p&gt;</content><author><name>Gerry Leo Nugroho</name></author><category term="Dataiku" /><summary type="html"></summary></entry><entry><title type="html">Kaggleâ€™s StumbleUpon Challenge with Dataiku</title><link href="http://localhost:4000/kaggles-stumbleupon-challenge-with-dataiku/" rel="alternate" type="text/html" title="Kaggleâ€™s StumbleUpon Challenge with Dataiku" /><published>2020-03-15T01:25:23+00:00</published><updated>2020-03-15T01:25:23+00:00</updated><id>http://localhost:4000/kaggles-stumbleupon-challenge-with-dataiku</id><content type="html" xml:base="http://localhost:4000/kaggles-stumbleupon-challenge-with-dataiku/">&lt;style&gt;.aligncenter {text-align: center;}&lt;/style&gt;

&lt;h3 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h3&gt;

&lt;p&gt;This Kaggle's &lt;a href=&quot;https://www.kaggle.com/c/stumbleupon&quot; title=&quot;StumbleUpon challenge&quot;&gt;StumbleUpon challenge&lt;/a&gt; has been around for sometimes by now, but I thought it'd be a great exercise to do them again, and this time we'll do it with the help of Dataiku platform. The idea is pretty much simple, how can we predict if an article getting submitted to StumbleUpon is an &quot;&lt;em&gt;ephemeral&lt;/em&gt;&quot; or an &quot;&lt;em&gt;evergreen&lt;/em&gt;&quot;. And as usual, Kaggle will provide you with both a train and test datasets from their site. Following paragraphs are simply taken from Kaggle's StumbleUpon page.&lt;/p&gt;

&lt;p&gt;StumbleUpon is a user-curated web content discovery engine that recommends relevant, high quality pages and media to its users, based on their interests. While some pages we recommend, such as news articles or seasonal recipes, are only relevant for a short period of time, others maintain a timeless quality and can be recommended to users long after they are discovered. In other words, pages can either be classified as &quot;&lt;em&gt;ephemeral&lt;/em&gt;&quot; or &quot;&lt;em&gt;evergreen&lt;/em&gt;&quot;. The ratings we get from our community give us strong signals that a page may no longer be relevant - but what if we could make this distinction ahead of time? A high quality prediction of &quot;&lt;em&gt;ephemeral&lt;/em&gt;&quot; or &quot;&lt;em&gt;evergreen&lt;/em&gt;&quot; would greatly improve a recommendation system like ours.&lt;/p&gt;

&lt;p&gt;Many people know evergreen content when they see it, but can an algorithm make the same determination without human intuition? Your mission is to build a classifier which will evaluate a large set of URLs and label them as either evergreen or ephemeral. Can you out-class(ify) StumbleUpon? As an added incentive to the prize, a strong performance in this competition may lead to a career-launching internship at one of the best places to work in San Francisco.&lt;/p&gt;

&lt;h3 id=&quot;2-prepare-recipes&quot;&gt;2. Data Dictionary&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;There are two components to the data provided for this challenge:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first component is two files: &lt;code&gt;train.tsv&lt;/code&gt; and &lt;code&gt;test.tsv&lt;/code&gt;. Each is a tab-delimited text file containing the fields outlined below for 10,566 urls total. Fields for which no data is available are indicated with a question mark.&lt;/p&gt;

&lt;p&gt;The first component is two files: &lt;code&gt;train.tsv&lt;/code&gt; and &lt;code&gt;test.tsv&lt;/code&gt;. Each is a tab-delimited text file containing the fields outlined below for 10,566 urls total. Fields for which no data is available are indicated with a question mark.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;train.tsv&lt;/code&gt; is the training set and contains 7,395 urls. Binary evergreen labels (either evergreen (1) or non-evergreen (0)) are provided for this set.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;test.tsv&lt;/code&gt; is the test/evaluation set and contains 3,171 urls.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The second component is &lt;code&gt;raw_content.zip&lt;/code&gt;, a zip file containing the raw content for each url, as seen by StumbleUpon&amp;rsquo;s crawler. Each url&amp;rsquo;s raw content is stored in a tab-delimited text file, named with the &lt;code&gt;urlid&lt;/code&gt; as indicated in &lt;code&gt;train.tsv&lt;/code&gt; and &lt;code&gt;test.tsv&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The following table includes field descriptions for &lt;code&gt;train.tsv&lt;/code&gt; and &lt;code&gt;test.tsv&lt;/code&gt;: By default, the initial dataset coming from &lt;a href=&quot;https://www.kaggle.com/c/stumbleupon/data%5D(https://www.kaggle.com/c/stumbleupon/data)&quot;&gt;Kaggle&amp;rsquo;s&lt;/a&gt; challenge page would give you the above dataset features at hand. But we&amp;rsquo;ll try to optimize them to something much more Machine Learning friendly looking dataset.&lt;/p&gt;


&lt;h3 id=&quot;3-dataflow&quot;&gt;3. Data Flow&lt;/h3&gt;
&lt;p&gt;My approach toward this is pretty much straightforward, as a first step, it's required for you to download the whole dataset and have them imported to your working Dataiku project space. I would assume you know your way around to navigate the Dataiku's interface and be able to have the project space and data importing properly registered. And here's how my current data workflow would look like.&lt;/p&gt;

&lt;p class=&quot;aligncenter&quot;&gt;
&lt;img data-src=&quot;/img/posts/stumbleupon-dataiku/screencap-01.jpg&quot; src=&quot;/img/posts/stumbleupon-dataiku/screencap-01.jpg&quot; class=&quot;img-fluid&quot; alt=&quot;stumble-upon&quot; &gt;
&lt;/p&gt;

&lt;h3 id=&quot;4-datacleaning&quot;&gt;3. Cleaning-up The Data&lt;/h3&gt;
&lt;p&gt;As with many other Machine Learning project out there, you wouldn't get a Model friendly dataset from the whole beginning of finding them. So our first objective would be to clean our dataset and turn them to a more machine learning friendly algorithm. Our current dataset's still requiring us to perform a lot of cleaning, since we can see that we still have unwanted characters much like a question mark, here and there.&lt;/p&gt;

&lt;p class=&quot;aligncenter&quot;&gt;
&lt;img data-src=&quot;/img/posts/stumbleupon-dataiku/screencap-02.jpg&quot; src=&quot;/img/posts/stumbleupon-dataiku/screencap-02.jpg&quot; class=&quot;img-fluid&quot; alt=&quot;stumble-upon&quot; &gt;
&lt;/p&gt;

&lt;h3 id=&quot;4-datacleaning&quot;&gt;4. Data Modeling&lt;/h3&gt;
&lt;p&gt;As this article would only be a short and sweet walkthough of the project, I'm not going to dive in further on each and every steps, and rather only focusing on the high level overview of the situation. Next, we go along with the data modeling of the dataset. Since the project is to predict whether or not a single URL submission would be a long lasting one or not, which on this case getting represented with the feature of 'label', it would than imply we're conducting a Two Class Classification machine learning algorithm.&lt;/p&gt;

&lt;p&gt;And for the case, Dataiku is shipped with the feature by default. Choose the 'Two Class Classification' from the drop down menu and pick the label as the target that we wish predict, as shown by the pciture below.&lt;/p&gt;

&lt;p class=&quot;aligncenter&quot;&gt;
&lt;img data-src=&quot;/img/posts/stumbleupon-dataiku/screencap-03.jpg&quot; src=&quot;/img/posts/stumbleupon-dataiku/screencap-03.jpg&quot; class=&quot;img-fluid&quot; alt=&quot;stumble-upon&quot; &gt;
&lt;/p&gt;


&lt;h3 id=&quot;4-datacleaning&quot;&gt;5. Running The Modeling&lt;/h3&gt;
&lt;p&gt;Once that we're satisfied with our previous result, we move into our next step, which involve in testing and running our previously define data modeling. Now this part the result may vary on your end, do to the fact, you might set the sampling method different with mine. I'm running the 'Logisitic Regression' algorithm for this case, which in returned delivered me a sufficient AUC (Area Under The Curve) ROC (Receiver Operating Characteristics) score.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;0.5 = This suggests no discrimination, so we might as well flip a coin.&lt;/li&gt;
&lt;li&gt;0.5-0.7 = We consider this poor discrimination, not much better than a coin toss.&lt;/li&gt;
&lt;li&gt;0.7-0.8 = Acceptable discrimination&lt;/li&gt;
&lt;li&gt;0.8-0.9= Excellent discrimination&lt;/li&gt;
&lt;li&gt;&gt;0.9 = Outstanding discrimination&quot;&lt;/li&gt;
&lt;/ul&gt;

&lt;p class=&quot;aligncenter&quot;&gt;
&lt;img data-src=&quot;/img/posts/stumbleupon-dataiku/screencap-04.jpg&quot; src=&quot;/img/posts/stumbleupon-dataiku/screencap-04.jpg&quot; class=&quot;img-fluid&quot; alt=&quot;stumble-upon&quot; &gt;
&lt;/p&gt;

&lt;h3 id=&quot;5-test-againts-the-test&quot;&gt;6. Run The Model Againts The Test Dataset&lt;/h3&gt;
&lt;p&gt;Now that we've settled our model's score, it's time for the actual action of the game, which is testing them againts our test dataset. Simply click on the Machine Learning algorithm icon on the dataflow, click the score icon on the right hand panel and choose the to test them againts the test dataset. It's pretty much by straight forward, so simply just accept the default configuration that Dataiku would offer you.&lt;/p&gt;

&lt;p class=&quot;aligncenter&quot;&gt;
&lt;img data-src=&quot;/img/posts/stumbleupon-dataiku/screencap-06.jpg&quot; src=&quot;/img/posts/stumbleupon-dataiku/screencap-06.jpg&quot; class=&quot;img-fluid&quot; alt=&quot;stumble-upon&quot; &gt;
&lt;/p&gt;


&lt;h3 id=&quot;5-test-againts-the-test&quot;&gt;7. The Result&lt;/h3&gt;
&lt;p&gt;And there you have it, as our modeling and algorithm from previous walked-through scenario tested againts the test dataset, it would  by the end of it, yield us the following results. Please scroll to the right and by each row of the test dataset, now is getting added with 3 additional coloumns. First would be the &lt;em&gt;proba_0&lt;/em&gt;, &lt;em&gt;proba_1&lt;/em&gt; and &lt;em&gt;prediction&lt;/em&gt;. Luckily, this dataset is somewhat relatively easier to work with, and by some minor tweaks it already giving us a decent final score to test our test dataset againts with.&lt;/p&gt;

&lt;p class=&quot;aligncenter&quot;&gt;
&lt;img data-src=&quot;/img/posts/stumbleupon-dataiku/screencap-07.jpg&quot; src=&quot;/img/posts/stumbleupon-dataiku/screencap-07.jpg&quot; class=&quot;img-fluid&quot; alt=&quot;stumble-upon&quot; &gt;
&lt;/p&gt;


&lt;h3 id=&quot;5-test-againts-the-test&quot;&gt;8. Submission to Kaggle&lt;/h3&gt;
&lt;p&gt;What comes next after those previous preparation of data modeling and testing, comes the actual submission phase. At the Kaggle site, they would provide you with a template of the accepted data submission format. We simply need to follow it, make a local copy of our results by downloading from the data flow diagram, and have them uploaded to Kaggle submission button.&lt;/p&gt;

&lt;p class=&quot;aligncenter&quot;&gt;
&lt;img data-src=&quot;/img/posts/stumbleupon-dataiku/screencap-08.jpg&quot; src=&quot;/img/posts/stumbleupon-dataiku/screencap-08.jpg&quot; class=&quot;img-fluid&quot; alt=&quot;stumble-upon&quot; &gt;
&lt;/p&gt;


&lt;h3 id=&quot;5-test-againts-the-test&quot;&gt;9. Kaggle Score &amp; Leaderboard&lt;/h3&gt;
&lt;p&gt;After the submission process you'll get the final score in returned from Kaggle, as with the current submission, we're only getting 0.75, which accoding to my opinion wasn't that so bad. Cause after all, we're only scracthing from the surface, and only from a high level overview. But rest asure you can always tweaks your model and improvise your model again and have them resubmited to Kaggle in ordernto get better final score. But as of now, that should do it as for this current article&lt;/p&gt;

&lt;p class=&quot;aligncenter&quot;&gt;
&lt;img data-src=&quot;/img/posts/stumbleupon-dataiku/screencap-09.jpg&quot; src=&quot;/img/posts/stumbleupon-dataiku/screencap-09.jpg&quot; class=&quot;img-fluid&quot; alt=&quot;stumble-upon&quot; &gt;
&lt;/p&gt;


&lt;h3 id=&quot;4-Jupyter Notebook&quot;&gt;10. Jupyter Notebook&lt;/h3&gt;
&lt;p&gt;Fell free to drop you comments and questions down below, and as always, I also put copies of the project at my &lt;a href=&quot;https://github.com/leonism/Dataiku-StumbleUpon&quot; title=&quot;Github repo StumbleUpon&quot;&gt;GitHub repo&lt;/a&gt;, fell free to download them as a single Zip file, and have them restore to your working Dataiku directory. Below are some Dataiku's Jupyter Notebook files I found interesting that we could always take lessons from, thank you for reading this article.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/leonism/Dataiku-StumbleUpon/blob/master/ipython_notebooks/Correlations%20analysis%20on%20test_prepared_scored%20(admin).ipynb&quot;&gt;Correlations analysis on test_prepared_scored (admin).ipynb
&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/leonism/Dataiku-StumbleUpon/blob/master/ipython_notebooks/PCA%20on%20test_prepared_scored%20(admin).ipynb&quot;&gt;PCA on test_prepared_scored (admin).ipynb&lt;/a&gt;
&lt;/li&gt;

&lt;li&gt;
&lt;a title=&quot;Predict label in train_prepared.ipynb&quot; href=&quot;https://github.com/leonism/Dataiku-StumbleUpon/blob/master/ipython_notebooks/Predict%20label%20in%20train_prepared.ipynb&quot;&gt;Predict label in train_prepared.ipynb&lt;/a&gt;]
&lt;/li&gt;

&lt;li&gt;
&lt;a href=&quot;https://github.com/leonism/Dataiku-StumbleUpon/blob/master/ipython_notebooks/Statistics%20and%20tests%20on%20a%20single%20population%20on%20test_prepared_scored%20(admin).ipynb&quot;&gt;Statistics and tests on a single population on test_prepared_scored (admin).ipynb&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Gerry Leo Nugroho</name></author><category term="Dataiku" /><summary type="html"></summary></entry><entry><title type="html">Sample Super Store Analysis Using Python &amp;amp; Pandas (Part 01)</title><link href="http://localhost:4000/super-sample-store-data-analysis-using-python-part-01/" rel="alternate" type="text/html" title="Sample Super Store Analysis Using Python &amp;amp; Pandas (Part 01)" /><published>2020-01-15T01:25:23+00:00</published><updated>2020-01-15T01:25:23+00:00</updated><id>http://localhost:4000/super-sample-store-data-analysis-using-python-part-01</id><content type="html" xml:base="http://localhost:4000/super-sample-store-data-analysis-using-python-part-01/">&lt;style&gt;.aligncenter {text-align: center;}&lt;/style&gt;

&lt;h3 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h1&gt;
&lt;p&gt;This tutorial is intended to give some quick, fundamental and brief overview on how to explore a dataset, which in this case is the Sample SuperStore. And to my best knowledge, it&amp;#39;s coming from a fictional e-commerce or online marketplace company&amp;#39;s annual sales figures. But the bottom line, the dataset would provide you with sufficient information on how you may work with an actual real life data. Among other things, this dataset consists of the following data types that may showcase the actual capabilities of what Pandas capable of delivering.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Timestamp&lt;/code&gt; or the timeseries datatypes.&lt;/li&gt;
&lt;li&gt;Both &lt;code&gt;Category&lt;/code&gt; and &lt;code&gt;SubCategory&lt;/code&gt; elements datatypes.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Numerical&lt;/code&gt; datatypes, so we may perform couple of numerical analysis.&lt;/li&gt;
&lt;li&gt;From the previous point, we sure can perform another features generation on this dataset.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And with that being said, let&amp;#39;s try to explore further the dataset, with the various built-in functionalities in the Panda&amp;#39;s library for Python have to offer, while utilizing the &lt;a href=&quot;https://jupyter.org/&quot;&gt;Jupyter Notebook&lt;/a&gt; as the main IDE of choice. And please don&amp;#39;t forget to download the CSV file used on this project, you can download it from my GitHub account found &lt;a href=&quot;https://github.com/leonism/sample-superstore/blob/master/data/superstore.csv&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;2-installation&quot;&gt;2. Installation&lt;/h1&gt;

&lt;p&gt;First and foremost, this data exploration would assume you, to have one proper and working installation of the &lt;a href=&quot;https://www.python.org/&quot;&gt;Python&lt;/a&gt; programming language. Secondly you have the &lt;a href=&quot;https://jupyter.org/&quot;&gt;Jupyter Notebook&lt;/a&gt; and the &lt;a href=&quot;https://pandas.pydata.org/&quot;&gt;Pandas&lt;/a&gt; library installed on your workstation, which we will use throughout the course of this data exploration.&lt;/p&gt;

&lt;p&gt;While the installation part for each of the mentioned pieces of software mentioned go beyond the scope of this tutorial, I would suggest that you head over to the official sites, and there you may discover further steps on how to download, install and setup the required programming language and libraries according to your operating systems. Once that you have settled with the whole installation and configuration issues, you may come back again to this page and follow along the instructions.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.python.org/&quot;&gt;Python Official Website&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://pandas.pydata.org/&quot;&gt;Panda&amp;#39;s Official Website&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://jupyter.org/&quot;&gt;Jupyter Official Website&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;3-importing-library&quot;&gt;3. Importing Library&lt;/h1&gt;
&lt;p&gt;Next, we need to load the Panda libraries onto our Jupyter Notebook environment. This way, it would tell Python, through the means available at Jupyter Notebook, to boot-up so we could utilize its built-ins functionalities available. Here&amp;#39;s a snippet on how to do load your Pandas to Jupyter Notebook environement:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;hljs-meta&quot;&gt;# Importing packages&lt;/span&gt;
&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; pandas &lt;span class=&quot;hljs-keyword&quot;&gt;as&lt;/span&gt; pd
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now, that we have settled our first challenge, let&amp;#39;s move further to read the datasets coming from a CSV file.&lt;/p&gt;

&lt;h3 id=&quot;4-reading-dataset&quot;&gt;4. Reading Dataset&lt;/h1&gt;
&lt;p&gt;The following code would imply these instructions&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;_&lt;code&gt;df_orders&lt;/code&gt;_ = is the name of the variable, that will be using throughout the example of this tutorial.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;code&gt;pd&lt;/code&gt;&lt;/em&gt; = stands for Panda, it&amp;#39;s the convention the community is using.&lt;/li&gt;
&lt;li&gt;_&lt;code&gt;.read_csv&lt;/code&gt;_ = is a method within to read the CSV file.&lt;/li&gt;
&lt;li&gt;_&lt;code&gt;index_col&lt;/code&gt;_ =&amp;#39;Order ID&amp;#39;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;hljs-comment&quot;&gt;# Let's try to read from the superstore.csv&lt;/span&gt;
&lt;span class=&quot;hljs-attr&quot;&gt;df_orders&lt;/span&gt; = pd.read_csv(&lt;span class=&quot;hljs-string&quot;&gt;'data/superstore.csv'&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If you notice form the above code, it&amp;#39;s implying that we need to put our &lt;code&gt;superstore.csv&lt;/code&gt; dataset on a directory called &lt;code&gt;data&lt;/code&gt;. So once that you download the dataset, create a folder named &lt;code&gt;data&lt;/code&gt; and put your &lt;code&gt;superstore.csv&lt;/code&gt; file there on that particular directory or folder.&lt;/p&gt;

&lt;p class=&quot;aligncenter&quot;&gt;
&lt;img data-src=&quot;/img/posts/super-sample-store-data-analysis-using-python-part-01/screencap-01.jpg&quot; src=&quot;/img/posts/super-sample-store-data-analysis-using-python-part-01/screencap-01.jpg&quot; class=&quot;img-fluid&quot; alt=&quot;super-sample-store-data-analysis-using-python&quot; &gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;caption text-muted&quot;&gt;This is the default view of a dataset in Jupyter Notebook&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;By default, Panda&amp;#39;s built-in functionality, &lt;strong&gt;only showing 20 columns and 10 rows&lt;/strong&gt; for each dataset, every time time you try to display them in the view. If you notice from the tabular data above, the dataset get truncated with triple dots sign &lt;strong&gt;&amp;#39;...&amp;#39;&lt;/strong&gt; both for the rows and the columns. And since this dataset has 10800 rows with 21 columns, it&amp;#39;ll only show the first 10 records for the row, with only 20 columns to the right instead of 21. As a side note, you can scroll the dataset both to the right and to the bottom, that way you can see the actual dataset content.&lt;/p&gt;

&lt;h3 id=&quot;5-dropping-the-row-id-&quot;&gt;5. Dropping The &amp;quot;Row ID&amp;quot;&lt;/h1&gt;
&lt;p&gt;The &lt;em&gt;&lt;code&gt;&amp;quot;Row ID&amp;quot;&lt;/code&gt;&lt;/em&gt; column is not really that informative, I think it would be safe enough for us to simply just delete them. That way, it would give us much more clarity over our dataset.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;_&lt;code&gt;df_orders&lt;/code&gt;_ = is the name of the variable, that will be using throughout the example of this tutorial.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;code&gt;.drop&lt;/code&gt;&lt;/em&gt; = the method being used to drop column.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;code&gt;inplace=True&lt;/code&gt;&lt;/em&gt; = we used them to keep the changes onward.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;
&lt;code&gt;df_orders.drop(&lt;span class=&quot;hljs-string&quot;&gt;&quot;Row ID&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-attr&quot;&gt;axis=1,&lt;/span&gt; &lt;span class=&quot;hljs-attr&quot;&gt;inplace=True)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;hljs-comment&quot;&gt;# Let's call the previously defined data variable, the 'df_orders'&lt;/span&gt;
&lt;span class=&quot;hljs-attribute&quot;&gt;df_orders&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;


&lt;p class=&quot;aligncenter&quot;&gt;
&lt;img data-src=&quot;/img/posts/super-sample-store-data-analysis-using-python-part-01/screencap-02.jpg&quot; src=&quot;/img/posts/super-sample-store-data-analysis-using-python-part-01/screencap-02.jpg&quot; class=&quot;img-fluid&quot; alt=&quot;super-sample-store-data-analysis-using-python&quot; &gt;
&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;caption text-muted&quot;&gt;The first column has been changed from &amp;#39;ROW ID&amp;#39; to &amp;#39;Order ID&amp;#39;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As you can see from the above table, we don&amp;#39;t have the &amp;#39;&lt;strong&gt;ROW ID&lt;/strong&gt;&amp;#39; no longer in place  and instead, it&amp;#39;s being replaced by the &amp;#39;&lt;strong&gt;Order ID&lt;/strong&gt;&amp;#39;.&lt;/p&gt;


&lt;h3 id=&quot;6-change-the-index-column&quot;&gt;6. Change &amp;quot;The Index&amp;quot; Column&lt;/h1&gt;
&lt;p&gt;By each time you&amp;#39;re using &lt;code&gt;pd.read_csv(&amp;#39;somedata.csv&amp;#39;)&lt;/code&gt;, that would yield the dataset&amp;#39;s actual rows and columns, and we certainly have quite an extensive records of data, as being displayed from the previous function.&lt;/p&gt;
&lt;p&gt;As you may notice, the first column isn&amp;#39;t the actual &lt;em&gt;&lt;code&gt;&amp;quot;Row ID&amp;quot;&lt;/code&gt;&lt;/em&gt; column, rather it&amp;#39;s the default built-in feature Panda&amp;#39;s bringing into the dataset. Let&amp;#39;s try to change that into something much more useful. Now, let&amp;#39;s revisit our previous Panda&amp;#39;s function, but this time we add another parameter, the _&lt;code&gt;&amp;quot;index_col&amp;quot;&lt;/code&gt;_ to be exact.&lt;/p&gt;
&lt;p&gt;Since that we wish to redo them again over a clean dataset, let&amp;#39;s just call them again one more time with the&lt;br&gt;
&lt;code&gt;&amp;#39;df_orders = pd.read_csv(&amp;#39;data/superstore.csv&amp;#39;, index_col=&amp;#39;Order ID&amp;#39;)&amp;#39;&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;Let&amp;#39;s continue with the &lt;code&gt;&amp;#39;df_orders&amp;#39;&lt;/code&gt; variable again. So don&amp;#39;t be surprise if you see the &amp;#39;&lt;code&gt;Row ID&amp;#39;&lt;/code&gt; column reappearing in the dataset since that would illustrate best our objective, but this time the &lt;em&gt;&amp;#39;index column&amp;#39;&lt;/em&gt; values have changed from the value coming from the &lt;code&gt;&amp;#39;Order ID&lt;/code&gt;&amp;#39; column instead.&lt;/p&gt;

&lt;pre&gt;
&lt;code&gt;&lt;span class=&quot;hljs-comment&quot;&gt;# Let's try to read again from the superstore.csv&lt;/span&gt;
&lt;span class=&quot;hljs-attr&quot;&gt;df_orders&lt;/span&gt; = pd.read_csv(&lt;span class=&quot;hljs-string&quot;&gt;'data/superstore.csv'&lt;/span&gt;, index_col=&lt;span class=&quot;hljs-string&quot;&gt;'Order ID'&lt;/span&gt;)
&lt;span class=&quot;hljs-comment&quot;&gt;# added the index_col='Order ID', parameter.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;hljs-comment&quot;&gt;# Let's call the previously defined data variable, the 'df_orders'&lt;/span&gt;

&lt;span class=&quot;hljs-attribute&quot;&gt;df_orders&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p class=&quot;aligncenter&quot;&gt;
&lt;img data-src=&quot;/img/posts/super-sample-store-data-analysis-using-python-part-01/screencap-03.jpg&quot; src=&quot;/img/posts/super-sample-store-data-analysis-using-python-part-01/screencap-03.jpg&quot; class=&quot;img-fluid&quot; alt=&quot;super-sample-store-data-analysis-using-python&quot; &gt;&lt;/p&gt;


&lt;p&gt;&lt;span class=&quot;caption text-muted&quot;&gt;The Index column has been changed, from default builtin, to &amp;#39;Order ID&amp;#39; column.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Once that we tried to add the additional parameter, as you may notice, the first column have changed to &amp;#39;&lt;code&gt;Order ID&amp;#39;&lt;/code&gt; column, rather then the previous Panda&amp;#39;s built-in index column, and the other thing was, the fine print below each table now have changed, from 21 columns, to only 20 columns instead.&lt;/p&gt;

&lt;h3 id=&quot;7-drop-the-row-id-change-the-index&quot;&gt;7. Drop the Row ID &amp;amp; Change The Index&lt;/h1&gt;
&lt;p&gt;On to our another objective, what if we wish to combine both of the features, with dropping the &lt;code&gt;&amp;#39;Row ID&lt;/code&gt;&amp;#39; and to change the &lt;code&gt;&amp;#39;Index&lt;/code&gt;&amp;#39; columns at the same time, so that we could get even leaner dataset to work with. With that kind of objective, we might need to combine both of the syntax to achive our objective.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df_orders.drop(&lt;span class=&quot;hljs-string&quot;&gt;&quot;Row ID&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-attr&quot;&gt;axis=&lt;/span&gt;&lt;span class=&quot;hljs-number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;hljs-attr&quot;&gt;inplace=&lt;/span&gt;&lt;span class=&quot;hljs-literal&quot;&gt;True&lt;/span&gt;, &lt;span class=&quot;hljs-attr&quot;&gt;index_col=&lt;/span&gt;'&lt;span class=&quot;hljs-keyword&quot;&gt;Order&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;ID&lt;/span&gt;')
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;.drop()&lt;/code&gt; = this method to drop a column from the dataset.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;axis=1&lt;/code&gt; = is the attribution value, on the dataset.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;inplace=True&lt;/code&gt; = we used them to keep our changes onward.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;index_col&lt;/code&gt; = make the defined column value, as our newly active index column instead.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;df_orders = pd.read_csv('data/superstore.csv', &lt;span class=&quot;hljs-attr&quot;&gt;index_col=&lt;/span&gt;'&lt;span class=&quot;hljs-keyword&quot;&gt;Order&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;ID&lt;/span&gt;')
df_orders.drop(&lt;span class=&quot;hljs-string&quot;&gt;&quot;Row ID&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-attr&quot;&gt;axis=&lt;/span&gt;&lt;span class=&quot;hljs-number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;hljs-attr&quot;&gt;inplace=&lt;/span&gt;&lt;span class=&quot;hljs-literal&quot;&gt;True&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;hljs-comment&quot;&gt;# Let's call the dataset again.&lt;/span&gt;
&lt;span class=&quot;hljs-attribute&quot;&gt;df_orders&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p class=&quot;aligncenter&quot;&gt;
&lt;img data-src=&quot;/img/posts/super-sample-store-data-analysis-using-python-part-01/screencap-04.jpg&quot; src=&quot;/img/posts/super-sample-store-data-analysis-using-python-part-01/screencap-04.jpg&quot; class=&quot;img-fluid&quot; alt=&quot;super-sample-store-data-analysis-using-python&quot; &gt;&lt;/p&gt;


&lt;p&gt;&lt;span class=&quot;caption text-muted&quot;&gt;The Index column has been changed, from default builtin, to &amp;#39;Order ID&amp;#39; column.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now, as you may see, from the below printed information, we only have 19 columns remaining left, coming from the initial 21 columns previously being shown. Now you may ask, &amp;quot;But how come it&amp;#39;s down to 19 columns, while we recall we only dropped 1 column?&amp;quot;. The answer to that is due to the &lt;code&gt;&amp;#39;index_col&lt;/code&gt;&amp;#39; method, whereas we define the &lt;code&gt;&amp;#39;Order ID&lt;/code&gt;&amp;#39; to settle as the Index of the dataset. Pandas doesn&amp;#39;t count that to a column, rather just another indexing attribution in the dataset.&lt;/p&gt;

&lt;h3 id=&quot;8-default-number-rows-columns&quot;&gt;8. Default Number Rows &amp;amp; Columns&lt;/h1&gt;
&lt;p&gt;Let&amp;#39;s try to set the maximum column and row to display, since by default the pandas library would display &lt;strong&gt;10 records&lt;/strong&gt; of rows in total for a single dataset. The first 5 would coming from the top records, and the remaining would be coming from the last 5 records as a whole.&lt;/p&gt;
&lt;p&gt;But that&amp;#39;s a little too much of information anyone could digest in a short glimpse, why don&amp;#39;t we just minimize them down to 5 records instead. The same thing with the columns view, whereas Pandas would display you 20 columns, but since our current dataset only have 19 of them, then that should be fine.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;_&lt;code&gt;pd.set_option(&amp;#39;display.max_columns&amp;#39;, 20)&lt;/code&gt;_ = setting the default column&amp;#39;s view.&lt;/li&gt;
&lt;li&gt;_&lt;code&gt;pd.set_option(&amp;#39;display.max_rows&amp;#39;, 5)&lt;/code&gt;_ = setting the default row&amp;#39;s view.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;hljs-comment&quot;&gt;# Let's try to read from the superstore.csv&lt;/span&gt;
pd.set_option(&lt;span class=&quot;hljs-string&quot;&gt;'display.max_columns'&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;20&lt;/span&gt;)
pd.set_option(&lt;span class=&quot;hljs-string&quot;&gt;'display.max_rows'&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;5&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;# Let's &lt;span class=&quot;hljs-keyword&quot;&gt;try&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;to&lt;/span&gt; give it a go &lt;span class=&quot;hljs-keyword&quot;&gt;with&lt;/span&gt; the &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; setting.
df_orders
&lt;/code&gt;&lt;/pre&gt;

&lt;p class=&quot;aligncenter&quot;&gt;
&lt;img data-src=&quot;/img/posts/super-sample-store-data-analysis-using-python-part-01/screencap-05.jpg&quot; src=&quot;/img/posts/super-sample-store-data-analysis-using-python-part-01/screencap-05.jpg&quot; class=&quot;img-fluid&quot; alt=&quot;super-sample-store-data-analysis-using-python&quot; &gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;caption text-muted&quot;&gt;Changing the default view in Pandas (illustration got truncated).&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;9-the-dataset-first-5-rows&quot;&gt;9. The Dataset First 5 Rows&lt;/h1&gt;
&lt;p&gt;Here&amp;#39;s another Pandas built-in method that may come handy. When you fell like taking a quick peek of the first 5 recrods from the top, the following code would deliver you those ouputs.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;_&lt;code&gt;df_orders&lt;/code&gt;_ = is the name of the variable, that will be using throughout the example of this tutorial.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;code&gt;.head()&lt;/code&gt;&lt;/em&gt; = is the method to display the first five records of data coming from the dataset.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;hljs-selector-tag&quot;&gt;df_orders&lt;/span&gt;&lt;span class=&quot;hljs-selector-class&quot;&gt;.head&lt;/span&gt;()
&lt;/code&gt;&lt;/pre&gt;

&lt;p class=&quot;aligncenter&quot;&gt;
&lt;img data-src=&quot;/img/posts/super-sample-store-data-analysis-using-python-part-01/screencap-06.jpg&quot; src=&quot;/img/posts/super-sample-store-data-analysis-using-python-part-01/screencap-06.jpg&quot; class=&quot;img-fluid&quot; alt=&quot;super-sample-store-data-analysis-using-python&quot; &gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;caption text-muted&quot;&gt;Viewing the first 5 rows of a dataset (illustration got truncated).&lt;/span&gt;&lt;/p&gt;

&lt;h3 id=&quot;10-the-dataset-last-5-rows&quot;&gt;10. The Dataset Last 5 Rows&lt;/h1&gt;
&lt;p&gt;Much like the above previous syntax, the similar can be apply to the bottom 5 records coming from your dataset. And you guess it right, the syntax would be &lt;code&gt;.tail()&lt;/code&gt; and that would give you the last 5 records from the dataset.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;df_orders&lt;/code&gt; = is the name of the variable, that will be using throughout the example of this tutorial.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.tail()&lt;/code&gt; = is the method to display the last five records of data coming from the dataset.&lt;/li&gt;
&lt;/ul&gt;



&lt;p class=&quot;aligncenter&quot;&gt;
&lt;img data-src=&quot;/img/posts/super-sample-store-data-analysis-using-python-part-01/screencap-07.jpg&quot; src=&quot;/img/posts/super-sample-store-data-analysis-using-python-part-01/screencap-07.jpg&quot; class=&quot;img-fluid&quot; alt=&quot;super-sample-store-data-analysis-using-python&quot; &gt;&lt;/p&gt;


&lt;p&gt;&lt;span class=&quot;caption text-muted&quot;&gt;Viewing the last 5 rows of a dataset (illustration got truncated).&lt;/span&gt;&lt;/p&gt;

&lt;h3 id=&quot;11-the-dataset-structure&quot;&gt;11. The Dataset Structure&lt;/h3&gt;
&lt;p&gt;Now that you have one finer understanding on the previous aspect of importing library, loading the dataset and manipulate the views of the rows and the columns, let&amp;#39;s now move on to the Dataset structure aspect. Whereas it&amp;#39;s also an important area, before continuing the journey of exploring the dataset further.&lt;/p&gt;
&lt;p&gt;The dataset you get from the wild, might not always have the proper structure and data types you need. And before you could do further analysis and manipulation, let&amp;#39;s make sure that both the structure and data types have been taken care of properly.&lt;/p&gt;
&lt;h3 id=&quot;12-rows-columns&quot;&gt;12. Rows &amp;amp; Columns&lt;/h3&gt;
&lt;p&gt;Following are both the built-in method to achive our next objective, as we go more deeper over the analysis part of the dataset. Let&amp;#39;s try to understand further of what how many rows and columns are there, we know this information from the previous part, but lucky for us, Pandas also provide us with a method di display the information in hand.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;_&lt;code&gt;df_orders&lt;/code&gt;_ = is the name of the variable, that will be using throughout the example of this tutorial.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;code&gt;.shape()&lt;/code&gt;&lt;/em&gt; = is the method to display the number of rows and column.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;hljs-selector-tag&quot;&gt;df_orders&lt;/span&gt;&lt;span class=&quot;hljs-selector-class&quot;&gt;.shape&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;(10800, 19)&lt;/p&gt;
&lt;p&gt;Aside from the fact, there are various other ways for us to know how many Rows and Columns available in your dataset, Pandas also has a builtin method to dispay those information. So now we understand that the dataset has the following total of information records.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;10800&lt;/code&gt; coloumns&lt;/li&gt;
&lt;li&gt;&lt;code&gt;19&lt;/code&gt; coloumns&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;13-dataset-columns&quot;&gt;13. Dataset Columns&lt;/h1&gt;
&lt;p&gt;Imagine that you&amp;#39;re working with a large dataset, and by large, not just merely on the amounts of rows that it&amp;#39;d produce. But also on the amount of columns spreaded from left to right. Good thing we&amp;#39;re only working a 19 columns (from previously 21 columns in our dataset), now wouldn&amp;#39;t it be nice to have a method to display all the columns available in our dataset? Well the good news is, Pandas shipped with a builtin method just to achive that.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;_&lt;code&gt;df_orders&lt;/code&gt;_ = is the name of the variable, that will be using throughout the example of this tutorial.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;code&gt;.columns&lt;/code&gt;&lt;/em&gt; = is the method to display all the columns available in the dataset.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;code&gt;.dtypes()&lt;/code&gt;&lt;/em&gt; = is the method to display the data types from the dataset available.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;# Let's &lt;span class=&quot;hljs-built_in&quot;&gt;print&lt;/span&gt; the &lt;span class=&quot;hljs-built_in&quot;&gt;columns&lt;/span&gt; (&lt;span class=&quot;hljs-built_in&quot;&gt;features&lt;/span&gt;) names.
df_orders.&lt;span class=&quot;hljs-built_in&quot;&gt;columns&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;Index&lt;/span&gt;([&lt;span class=&quot;hljs-string&quot;&gt;'Order Date'&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;'Ship Date'&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;'Ship Mode'&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;'Customer ID'&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;'Customer Name'&lt;/span&gt;,
       &lt;span class=&quot;hljs-string&quot;&gt;'Segment'&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;'Country'&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;'City'&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;'State'&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;'Postal Code'&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;'Region'&lt;/span&gt;,
       &lt;span class=&quot;hljs-string&quot;&gt;'Product ID'&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;'Category'&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;'Sub-Category'&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;'Product Name'&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;'Sales'&lt;/span&gt;,
       &lt;span class=&quot;hljs-string&quot;&gt;'Quantity'&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;'Discount'&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;'Profit'&lt;/span&gt;],
      dtype=&lt;span class=&quot;hljs-string&quot;&gt;'object'&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;14-renaming-columns&quot;&gt;14. Renaming Columns&lt;/h3&gt;
&lt;p&gt;Now it&amp;#39;s just my way of doing things over the EDA aspect, is trying to eliminate any white spaces in between the columns name. It gets better and you may benefit alot from performing this method, and as we move on to something that&amp;#39;s much more complicated within the data exploration, we&amp;#39;ll benefit on the clarity aspect, too.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;_&lt;code&gt;df_orders&lt;/code&gt;_ = is the name of the variable, that will be using throughout the example of this tutorial.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;code&gt;.columns&lt;/code&gt;&lt;/em&gt; = is the method to rename the column names.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;hljs-comment&quot;&gt;# Let's try to rename the column.&lt;/span&gt;
df_orders.columns = [&lt;span class=&quot;hljs-string&quot;&gt;'OrderDate'&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;'ShipDate'&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;'ShipMode'&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;'CustomerID'&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;'CustomerName'&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;'Segment'&lt;/span&gt; , &lt;span class=&quot;hljs-string&quot;&gt;'Country'&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;'City'&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;'State'&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;'PostalCode'&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;'Region'&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;'ProductID'&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;'Category'&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;'SubCategory'&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;'ProductName'&lt;/span&gt; , &lt;span class=&quot;hljs-string&quot;&gt;'Sales'&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;'Quantity'&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;'Discount'&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;'Profit'&lt;/span&gt;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;df_orders&lt;/code&gt;&lt;/pre&gt;


&lt;p class=&quot;aligncenter&quot;&gt;
&lt;img data-src=&quot;/img/posts/super-sample-store-data-analysis-using-python-part-01/screencap-11.jpg&quot; src=&quot;/img/posts/super-sample-store-data-analysis-using-python-part-01/screencap-11.jpg&quot; class=&quot;img-fluid&quot; alt=&quot;super-sample-store-data-analysis-using-python&quot; &gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;caption text-muted&quot;&gt;The columns after being renamed. (illustration got truncated).&lt;/span&gt;&lt;/p&gt;

&lt;h3 id=&quot;15-columns-data-type&quot;&gt;15. Columns Data Type&lt;/h1&gt;
&lt;p&gt;Many times before you wish to explore further your columns in a dataset with some operation, you may need to make sure it&amp;#39;s the correct data type that you&amp;#39;re working with. For example, you wouldn&amp;#39;t be able to do a division operation over a Timestamp data format, or multiply a String with with another string for that matter.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;_&lt;code&gt;df_orders&lt;/code&gt;_ = is the name of the variable, that will be using throughout the example of this tutorial.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;code&gt;.columns&lt;/code&gt;&lt;/em&gt; = is the method to display all the columns available in the dataset.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;code&gt;.dtypes()&lt;/code&gt;&lt;/em&gt; = is the method to display the data types from the dataset available.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;hljs-comment&quot;&gt;# Let's print the columns data types.&lt;/span&gt;
df_orders.&lt;span class=&quot;hljs-keyword&quot;&gt;info&lt;/span&gt;()
&lt;/code&gt;&lt;/pre&gt;


&lt;p class=&quot;aligncenter&quot;&gt;
&lt;img data-src=&quot;/img/posts/super-sample-store-data-analysis-using-python-part-01/screencap-12.jpg&quot; src=&quot;/img/posts/super-sample-store-data-analysis-using-python-part-01/screencap-12.jpg&quot; class=&quot;img-fluid&quot; alt=&quot;super-sample-store-data-analysis-using-python&quot; &gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;caption text-muted&quot;&gt;Viewing the data types of the dataset (illustration got truncated).&lt;/span&gt;&lt;/p&gt;

&lt;h3 id=&quot;16-incorrect-columns-data-type&quot;&gt;16. Incorrect Columns Data Type&lt;/h1&gt;
&lt;p&gt;As we can see from the above snippets, we have noticed there are couple of columns data types that were set incorrectly. For instance, the &lt;code&gt;OrderDate&lt;/code&gt; data type was set to &lt;code&gt;object&lt;/code&gt; data type instead of &lt;code&gt;datetime&lt;/code&gt;, or the &lt;code&gt;PostalCode&lt;/code&gt; was set to &lt;code&gt;float&lt;/code&gt; data type, though you wouldn&amp;#39;t do any calculation on top of it. Somewhere down the line with that kind of flaws, will lead us to even bigger problem if we don&amp;#39;t try to fix them now. Let&amp;#39;s try to patch those data types with the following methods.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;_&lt;code&gt;df_orders&lt;/code&gt;_ = is the name of the variable, that will be using throughout the example of this tutorial.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;code&gt;.astype&lt;/code&gt;&lt;/em&gt; = is the method to change the coloumns data type in the dataset.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;hljs-comment&quot;&gt;# Let's try to change the datatypes of the following column in the dataset.&lt;/span&gt;
df_orders[&lt;span class=&quot;hljs-string&quot;&gt;'OrderDate'&lt;/span&gt;] = df_orders[&lt;span class=&quot;hljs-string&quot;&gt;'OrderDate'&lt;/span&gt;].astype(&lt;span class=&quot;hljs-string&quot;&gt;'datetime64[ns]'&lt;/span&gt;)
df_orders[&lt;span class=&quot;hljs-string&quot;&gt;'ShipDate'&lt;/span&gt;] = df_orders[&lt;span class=&quot;hljs-string&quot;&gt;'ShipDate'&lt;/span&gt;].astype(&lt;span class=&quot;hljs-string&quot;&gt;'datetime64[ns]'&lt;/span&gt;)
df_orders[&lt;span class=&quot;hljs-string&quot;&gt;'PostalCode'&lt;/span&gt;] = df_orders[&lt;span class=&quot;hljs-string&quot;&gt;'PostalCode'&lt;/span&gt;].astype(&lt;span class=&quot;hljs-string&quot;&gt;'object'&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And now let&amp;#39;s try to recheck them again, to see if the codes have worked as intended.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;hljs-comment&quot;&gt;# Let's print the columns data types.&lt;/span&gt;
df_orders.&lt;span class=&quot;hljs-keyword&quot;&gt;info&lt;/span&gt;()
&lt;/code&gt;&lt;/pre&gt;

&lt;p class=&quot;aligncenter&quot;&gt;
&lt;img data-src=&quot;/img/posts/super-sample-store-data-analysis-using-python-part-01/screencap-10.jpg&quot; src=&quot;/img/posts/super-sample-store-data-analysis-using-python-part-01/screencap-10.jpg&quot; class=&quot;img-fluid&quot; alt=&quot;super-sample-store-data-analysis-using-python&quot; &gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;caption text-muted&quot;&gt;We&amp;#39;ve now changed the data types.&lt;/span&gt;&lt;/p&gt;

&lt;h3 id=&quot;17-dataset-statistic-figures&quot;&gt;17. Dataset Statistic Figures&lt;/h1&gt;
&lt;p&gt;If you&amp;#39;re more into the statistician type, perhaps you may be also interested by the following methods to generate the figures with only a single line of &lt;code&gt;.describe&lt;/code&gt; Pandas method.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;_&lt;code&gt;df_orders&lt;/code&gt;_ = is the name of the variable, that will be using throughout the example of this tutorial.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;code&gt;.describe&lt;/code&gt;&lt;/em&gt; = is the method to pull out some statistics figures from the dataset.&lt;/li&gt;
&lt;li&gt;Short note, the &lt;code&gt;.describe&lt;/code&gt; method would only work for numerical coloumn, and not categorical.&lt;/li&gt;
&lt;li&gt;While for the &lt;code&gt;(include=&amp;#39;all&amp;#39;)&lt;/code&gt;, would work on both numerical &amp;amp; categorical values.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;hljs-comment&quot;&gt;# Describing statistical information on the dataset&lt;/span&gt;
&lt;span class=&quot;hljs-attribute&quot;&gt;df_orders&lt;/span&gt;.describe()
&lt;/code&gt;&lt;/pre&gt;

&lt;p class=&quot;aligncenter&quot;&gt;&lt;img data-src=&quot;/img/posts/super-sample-store-data-analysis-using-python-part-01/screencap-13.jpg&quot; src=&quot;/img/posts/super-sample-store-data-analysis-using-python-part-01/screencap-13.jpg&quot; class=&quot;img-fluid&quot; alt=&quot;super-sample-store-data-analysis-using-python&quot; &gt;&lt;/p&gt;


&lt;p&gt;&lt;span class=&quot;caption text-muted&quot;&gt;Decent Statistic Facts on The Dataset.&lt;/span&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;hljs-comment&quot;&gt;# Describing more statistical information on the dataset&lt;/span&gt;
df_orders.describe(&lt;span class=&quot;hljs-keyword&quot;&gt;include&lt;/span&gt;=&lt;span class=&quot;hljs-string&quot;&gt;'all'&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;


&lt;p class=&quot;aligncenter&quot;&gt;&lt;img data-src=&quot;/img/posts/super-sample-store-data-analysis-using-python-part-01/screencap-14.jpg&quot; src=&quot;/img/posts/super-sample-store-data-analysis-using-python-part-01/screencap-14.jpg&quot; class=&quot;img-fluid&quot; alt=&quot;super-sample-store-data-analysis-using-python&quot; &gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;caption text-muted&quot;&gt;Decent Statistic Facts on The Dataset.&lt;/span&gt;&lt;/p&gt;

&lt;h3 id=&quot;18-statistic-figures&quot;&gt;18. Statistic Figures&lt;/h1&gt;
&lt;p&gt;The following code would imply these instructions&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;_&lt;code&gt;df_orders&lt;/code&gt;_ = is the name of the variable, that will be using throughout the example of this tutorial.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;code&gt;.count&lt;/code&gt;&lt;/em&gt; = is the count value to a specific column.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;code&gt;.mean&lt;/code&gt;&lt;/em&gt; = is the std value to a specific column.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;code&gt;.min&lt;/code&gt;&lt;/em&gt; = is the min value to a specific column.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;df_orders[&lt;span class=&quot;hljs-string&quot;&gt;&quot;Sales&quot;&lt;/span&gt;].&lt;span class=&quot;hljs-built_in&quot;&gt;count&lt;/span&gt;()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Would give you the value of &lt;code&gt;&amp;quot;9994&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;hljs-selector-tag&quot;&gt;df_orders&lt;/span&gt;&lt;span class=&quot;hljs-selector-attr&quot;&gt;[&quot;Sales&quot;]&lt;/span&gt;&lt;span class=&quot;hljs-selector-class&quot;&gt;.mean&lt;/span&gt;()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Would give you the value of &lt;code&gt;&amp;quot;229.8580008304938&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df_orders[&lt;span class=&quot;hljs-string&quot;&gt;&quot;Sales&quot;&lt;/span&gt;].&lt;span class=&quot;hljs-built_in&quot;&gt;std&lt;/span&gt;()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Would give you the value of &lt;code&gt;&amp;quot;623.2451005086818&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df_orders[&lt;span class=&quot;hljs-string&quot;&gt;&quot;Sales&quot;&lt;/span&gt;].&lt;span class=&quot;hljs-built_in&quot;&gt;min&lt;/span&gt;()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Would give you the value of &lt;code&gt;&amp;quot;0.444&amp;quot;&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;19-exporting-dataset&quot;&gt;19. Exporting Dataset&lt;/h1&gt;
&lt;p&gt;Once that we&amp;#39;ve satisfied with our results, it&amp;#39;s time to export them. So let&amp;#39;s export them a new CSV dataset, so we could work with them on the next notebook tutorial.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;_&lt;code&gt;df_orders&lt;/code&gt;_ = is the name of the variable, that will be using throughout the example of this tutorial.&lt;/li&gt;
&lt;li&gt;_&lt;code&gt;.to_csv&lt;/code&gt;_ = = is the export method to a CSV dataset.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;code&gt;index = False&lt;/code&gt;&lt;/em&gt; = we need to definet this index value set to False, since we don&amp;#39;t want the index column.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;df_orders.to_csv(&lt;span class=&quot;hljs-string&quot;&gt;'data/df_orders_exported.csv'&lt;/span&gt;, &lt;span class=&quot;hljs-keyword&quot;&gt;index&lt;/span&gt; =&lt;span class=&quot;hljs-keyword&quot;&gt;False&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now go ahead and check your current working directory. you may find your &lt;code&gt;df_orders_exported.csv&lt;/code&gt; there.&lt;/p&gt;

&lt;h3 id=&quot;20-bonus-stage&quot;&gt;20. Bonus Stage&lt;/h1&gt;
&lt;p&gt;Now that we&amp;#39;ve come a long way of exploring our &lt;code&gt;superstore.csv&lt;/code&gt; dataset, it&amp;#39;s time to dive a little bit deeper of what, both Python and Pandas capable of delivering. Let&amp;#39;s try to create a custom class in Python by leveraging our builtin Pandas method available in the library.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&lt;span class=&quot;hljs-comment&quot;&gt;# Let's create a class named `display_all`, by which later we call on the next command.&lt;/span&gt;
&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;display_all&lt;/span&gt;
&lt;span class=&quot;hljs-params&quot;&gt;(df_orders)&lt;/span&gt;:&lt;/span&gt;
&lt;span class=&quot;hljs-keyword&quot;&gt;with&lt;/span&gt; pd.option_context(&lt;span class=&quot;hljs-string&quot;&gt;&quot;display.max_rows&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;1000&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;display.max_columns&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;1000&lt;/span&gt;):
        display(df_orders)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What it does basically, it creates a class named &lt;code&gt;display_all&lt;/code&gt; and called the &lt;code&gt;df_orders&lt;/code&gt; variable that we&amp;#39;ve defined earlier at the top of this jupyter notebook tutorial. Next, we call the &lt;code&gt;pd.option_context&lt;/code&gt; method that would provide us with the &lt;code&gt;display.max_rows&lt;/code&gt; and the &lt;code&gt;dispplay.max_columns&lt;/code&gt; attributions. And lastly we combine them all in the &lt;code&gt;display()&lt;/code&gt; method by the end of the class.&lt;/p&gt;
&lt;p&gt;Now that we&amp;#39;ve combine them all together, let&amp;#39;s put them into action, and see what it delivers back to us this time.&lt;/p&gt;

&lt;pre&gt;
&lt;code&gt;display_all(&lt;span class=&quot;hljs-name&quot;&gt;df_orders&lt;/span&gt;.head(&lt;span class=&quot;hljs-number&quot;&gt;10&lt;/span&gt;).T)
&lt;/code&gt;
&lt;/pre&gt;


&lt;p class=&quot;aligncenter&quot;&gt;&lt;img data-src=&quot;/img/posts/super-sample-store-data-analysis-using-python-part-01/screencap-15.jpg&quot; src=&quot;/img/posts/super-sample-store-data-analysis-using-python-part-01/screencap-15.jpg&quot; class=&quot;img-fluid&quot; alt=&quot;super-sample-store-data-analysis-using-python&quot; &gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;caption text-muted&quot;&gt;Now the dataset getting transposed in the view.&lt;/span&gt;&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;display_all(&lt;span class=&quot;hljs-name&quot;&gt;df_orders&lt;/span&gt;.describe(&lt;span class=&quot;hljs-name&quot;&gt;include=&lt;/span&gt;'all').T)
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Since we&amp;#39;ve defined the &lt;code&gt;display_all&lt;/code&gt; from the previous class, we can now use it to explore further and combine them with different methods available in Pandas, much like the &lt;code&gt;.describe&lt;/code&gt; attribute.&lt;/p&gt;

&lt;p class=&quot;aligncenter&quot;&gt;
&lt;img src=&quot;/img/posts/super-sample-store-data-analysis-using-python-part-01/screencap-16.jpg&quot; alt=&quot;super-sample-store-data-analysis-using-python&quot;&gt;
&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;caption text-muted&quot;&gt;Now the Statistic from the dataset getting transposed in the view.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;You&amp;#39;ve made it this far, congratulations on achieving your first essential data-scienctist project. But there&amp;#39;s more, if you wish to explore further or perhaps want to experiment, fork this Jupyter Notebook else even copy them to your working directory. I made everthing available on my &lt;a href=&quot;https://github.com/leonism/sample-superstore&quot;&gt;GitHub repository&lt;/a&gt;. Got any questions? Fell free to ask them down below in the comment section, I&amp;#39;ll try to get back to any inquiries as soon as I could. Hope you enjoy this tutorial, and thank you for reading them.&lt;/p&gt;</content><author><name>Gerry Leo Nugroho</name></author><category term="Python-Pandas" /><summary type="html"></summary></entry></feed>